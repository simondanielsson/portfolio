{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization using Byte-Pair Encoding and a Unigram Language Model\n",
    "\n",
    "Author: Pierre Nugues with help from Marcus Klang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will create a tokenization program to handle subwords.\n",
    "\n",
    "In many scripts from Asia, like Chinese, Korean, or Japanese scripts, tokenization cannot rely on white spaces. The byte-pair encoding and the unigram language model are techniques that are now common in machine translation to carry out a tokenization at a subword level. Subword level tokenization shows better multilingual capabilities.\n",
    "\n",
    "You will follow two papers: \n",
    "* Subword Regularization: _Improving Neural Network Translation Models with Multiple Subword Candidates_ by Kudo (2018) (https://arxiv.org/pdf/1804.10959.pdf) and \n",
    "* _Byte Pair Encoding is Suboptimal for Language Model Pretraining_ by Bostrom and Durrett (2020) (https://aclanthology.org/2020.findings-emnlp.414.pdf). \n",
    "\n",
    "In addition, you will start from a clear and easy-to-understand description in Google’s Neural Machine Translation System: _Bridging the Gap between Human and Machine Translation_ by Wu et al. (2016). (Do not read them now)\n",
    "https://arxiv.org/abs/1609.08144\n",
    "\n",
    "You will use a small corpus make it easier to test and correct your code. Note also that you will use _characters_ and not _bytes_ in this lab as this is simpler to implement. For a complete program, see the link at the end.\n",
    "\n",
    "**In your report, be sure to answer all the questions. Please reuse the section titles of this notebook so that I can check your answers more easily**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an overall description of the subword tokenizers, read Sections 4 (introduction paragraph) and 4.1. in the paper on translation: _Bridging the Gap between Human and Machine Translation_ by Wu et al. (2016), https://arxiv.org/abs/1609.08144.  \n",
    "\n",
    "In your report, in a few lines (10 to 15 lines or so) you will:\n",
    "\n",
    "1. Outline the difference with tokenization as you saw it during the course;\n",
    "2. Imagine how the tokens will be learned (this will developed in the rest of the lab);\n",
    "3. Summarize what could be the advantages for Asian languages, unknown words, and translation.\n",
    "\n",
    "Commenting Sections 4 and 4.1 in your report is **mandatory**. If you are curious, you can read the complete article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of the BPE Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first algorithm to build the subwords from a corpus is a byte-pair encoding (BPE), due to Gage (1994). In the lab, you will first read two sections of more recent articles as they are easier to understand and specifically targeted to natural language processing.\n",
    "\n",
    "Read these two sections:\n",
    "\n",
    "1. Section 3.1 of _Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates_ (https://arxiv.org/pdf/1804.10959.pdf) by Kudo (2018).\n",
    "2. Section 2, algorithm 1 of _Byte Pair Encoding is Suboptimal for Language Model Pretraining_ (https://aclanthology.org/2020.findings-emnlp.414.pdf) by Bostrom and Durrett (2020).\n",
    "\n",
    "In your report, **summarize** (10 to 15 lines or so) with your own words the byte-pair encoding (BPE) algorithm as described by Kudo (2018) and Bostrom and Durrett (2020) (Only BPE and not the unigram language model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now program a byte-pair encoding program in Python. You will do it step by step. The first part will be to extract the subwords from a corpus. Note that you will use the characters, not the bytes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import tqdm as tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First use a small corpus and then, if you have time, test your program on a larger one. Here we take the smallest novel from Selma Lagerlöf in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "\n",
    "# Parameters for Selma dataset\n",
    "SELMA_URL = \"https://github.com/pnugues/ilppp/raw/master/programs/corpus/Selma.zip\"\n",
    "\n",
    "SELMA_FILES = [\n",
    "    os.path.join(\"Selma\", fname) \n",
    "    for fname in \n",
    "    [\n",
    "        \"bannlyst.txt\", \n",
    "        \"gosta.txt\", \n",
    "        \"herrgard.txt\", \n",
    "        \"jerusalem.txt\", \n",
    "        \"kejsaren.txt\", \n",
    "        \"marbacka.txt\", \n",
    "        \"nils.txt\", \n",
    "        \"osynliga.txt\", \n",
    "        \"troll.txt\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "def download_and_extract_selma():\n",
    "    \"\"\"Downloads and unpacks Selma.zip\"\"\"\n",
    "    \n",
    "    # Download if not all files exist\n",
    "    req = requests.get(SELMA_URL, stream=True)\n",
    "    if req.status_code != 200:\n",
    "        print(\"Failed to download file, got status: \" + req.status_code)\n",
    "        req.close()\n",
    "    else:\n",
    "        with open(\"Selma.zip\", \"wb\") as fd:\n",
    "            written = 0\n",
    "            for chunk in req.iter_content(chunk_size=65536):\n",
    "                fd.write(chunk)\n",
    "                written += len(chunk)\n",
    "                print(\"Downloading: %d bytes written to Selma.zip\" % written)\n",
    "\n",
    "        print(\"Selma.zip donwnloaded.\")\n",
    "        req.close()\n",
    "        \n",
    "        selma_zipfile = ZipFile(\"Selma.zip\")\n",
    "        selma_files_to_extract = [zi for zi in selma_zipfile.filelist if not zi.filename.startswith(\"__\") and zi.filename.endswith(\".txt\")]\n",
    "        for zi in selma_files_to_extract:\n",
    "            selma_zipfile.extract(zi)\n",
    "            print(\"Extracted: \" + zi.filename)\n",
    "            \n",
    "        print(\"Done!\")\n",
    "        \n",
    "# If not all path exists (all are true), then download\n",
    "if not all([os.path.exists(fname) for fname in SELMA_FILES]):\n",
    "    download_and_extract_selma()\n",
    "else:\n",
    "    print(\"Selma has been downloaded.\")\n",
    "    \n",
    "SELMA_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILE_PATH = '../../corpus/Selma.txt'\n",
    "FILE_PATH = 'Selma/herrgard.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the corpus and store it in the `corpus` string variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE_PATH, encoding='utf8') as f:\n",
    "    corpus = f.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all the space sequences in `corpus`, including newlines and tabulations, and normalize them as one space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "corpus = re.sub('\\p{Whitespace}+', \" \", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code (one instruction) to split the corpus in a list of characters and store the results in `corpus_l`. This is just a type conversion. Given the input:\n",
    "<pre><span style=\"font-size: 12pt;\">corpus = 'De senaste fem &aring;ren har cirka 25 000 unga'</span></pre>\n",
    "\n",
    "Return:\n",
    "<pre><span style=\"font-size: 12pt;\">corpus_l = ['D', 'e', ' ', 's', 'e', 'n', 'a', 's', 't', 'e', ' ', 'f', 'e', 'm', ' ', ...]</span></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "corpus_l = list(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_l[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the set of characters that will serve as initial subword tokens:\n",
    "\n",
    "1. Write a statement to extract the set of all the characters from `corpus_l`; \n",
    "2. Exclude the space from this set and call the resulting set: `char_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "char_set = set(corpus_l) - set(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(char_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using code from the previous question, write an `initial_vocabulary()` function taking the the `corpus_l` variable as input and returning the the set of all characters appearing in the corpus (the initial character set), deprived from the white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def initial_vocabulary(corpus_l):\n",
    "    return set(corpus_l) - set(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_vocabulary(corpus_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a `pair_count()` function that takes a list of tokens as input, possibly single characters or subword tokens, and that counts the adjacent pairs (bigrams). You will implement these counts as dictionaries: The key will be a pair (tuple) of adjacent symbols and the value, its frequency. Remember that you cannot cross whitespaces, i.e. a pair cannot include a whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the input\n",
    "\n",
    "`['D', 'e', ' ', 's', 'e', 'n', 'a', 's', 't', ...]`\n",
    "count_pairs should return a dictionary: \n",
    "\n",
    "\n",
    "`{('D', 'e'): 1, ('s', 'e'): 1, ('e', 'n'): 1, ('n', 'a'): 1, ...}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "from collections import Counter\n",
    "def pair_count(corpus_l):\n",
    "    pairs = []\n",
    "    for c1, c2 in zip(corpus_l[:-1], corpus_l[1:]):\n",
    "        if c1 == ' ' or c2 == ' ':\n",
    "            continue\n",
    "        pairs += [(c1, c2)]\n",
    "\n",
    "    return dict(Counter(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pair_count(corpus_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the most frequent pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code\n",
    "most_freq_pair = max(pairs, key= pairs.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join(most_freq_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The First Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the initial symbols in a `vocabulary` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = initial_vocabulary(corpus_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your most frequent pair to the vocabulary after one iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "def add_to_voc(corpus_l):\n",
    "    pairs = pair_count(corpus_l)\n",
    "    most_freq_pair = ''.join(max(pairs, key= pairs.get))\n",
    "    return most_freq_pair\n",
    "\n",
    "extended_voc = vocabulary.union({add_to_voc(corpus_l)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extended_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental Construction\n",
    "We will now incrementally build the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `merge_bigrams()` function that takes a list of tokens, `corpus_l`, and a pair of subword tokens `(token_r, token_l)` as input and merges adjacent sequences token_r, token_l into a new token, `token_new`, replacing the sequence `token_r, token_l` in `corpus_l`. Your function will return a new list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the input \n",
    "\n",
    "`corpus_l = ['D', 'e', ' ', 's', 'e', 'n', 'a', 's', 't', ...]`\n",
    "\n",
    "`merge_bigrams(corpus_l, ('e', 'n'))` should return where all the seuquences of 'e' and 'n' have been merged:\n",
    "\n",
    "`['D', 'e', ' ', 's', 'en', 'a', 's', 't', ...]`\n",
    "\n",
    "And reapplying `merge_bigrams(corpus_l, ('s', 'en'))` to this corpus should return\n",
    "\n",
    "`['D', 'e', ' ', 'sen', 'a', 's', 't', ...]`\n",
    "\n",
    "You will apply a greedy algorithm. Given the pair ('a', 'a') and the list ['a', 'a', 'a'], the result will be: ['aa', 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "class Remove:\n",
    "    pass\n",
    "\n",
    "def merge_bigrams(corpus_l, pair):\n",
    "    new_corpus = corpus_l.copy()\n",
    "    skip_index = -1\n",
    "    for index, this_pair in enumerate(zip(corpus_l[:-1], corpus_l[1:])):\n",
    "        if this_pair == pair:\n",
    "            new_corpus[index] = ''.join(this_pair)\n",
    "            new_corpus[index + 1] = \"REMOVEME\"\n",
    "        \n",
    "    return [element for element in new_corpus if  element != \"REMOVEME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = ['D', 'e', ' ', 's', 'e', 'n', 'a', 's', 't']\n",
    "merge_bigrams(corpus_test, ('e', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_bigrams(merge_bigrams(corpus_test, ('e', 'n')), ('s', 'en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Byte Pair Encoding (BPE): Building the Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write now a `BPE()` function following Algorithm 1 in _Byte Pair Encoding is Suboptimal for Language Model Pretraining_ by Bostrom and Durrett (2020). \n",
    "\n",
    "Your function will take `corpus_l` and the vocabulary size `k` as input. This size `k` will correspond to the count of new subwords added to the initial list of symbols. With your initial corpus, you should have 67 found symbols. With `k = 10`, you will add 10 subwords to this initial list. Note that Bostrom and Durrett (2020) define their $k_\\text{Bostrom and Durrett}$ as `k + initial vocabulary`. \n",
    "\n",
    "Return the vocabulary of subword tokens in the form of a list: the initial vocabulary and the subwords you will create.\n",
    "\n",
    "You will start from the initial vocabulary and `k` will be the number of symbols you add to this vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def BPE(corpus_l, k=10, verbose=True):\n",
    "    vocabulary = initial_vocabulary(corpus_l)\n",
    "    print(len(vocabulary))\n",
    "    \n",
    "    corpus = corpus_l.copy()\n",
    "    for _ in tqdm.tqdm(range(k)):\n",
    "        # find most common bigram\n",
    "        pair_counts = pair_count(corpus)\n",
    "        most_freq_pair = max(pair_counts, key=pair_counts.get)\n",
    "        \n",
    "        # update corpus\n",
    "        corpus = merge_bigrams(corpus, most_freq_pair)\n",
    "        # update vocabulary\n",
    "        new_token = ''.join(most_freq_pair)\n",
    "        vocabulary = vocabulary.union({new_token})\n",
    "        print(new_token, end=' ') if verbose else None\n",
    "    \n",
    "    del corpus\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a vocabulary of 50 subwords in addition to our initial set of symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = BPE(corpus_l, 50)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now use the vocabulary you obtained to tokenize a text stored in the corpus string.\n",
    "\n",
    "You will implement a greedy technique building on Python's regular expression engine. You will call this function `tokenize_bpe()` that will take two inputs: `corpus` and `vocabulary`, and that will return the tokenized text in the form of a list.\n",
    "\n",
    "    def tokenize_bpe(corpus, vocabulary):\n",
    "\n",
    "      ...\n",
    "\n",
    "      return tokens\n",
    "Here are a few hints on how to write this function. Before you call a regular expression and apply it to a text, a regex engine compiles it into an efficient automaton (you do not need to call `compile()` as the automaton is automatically cached). The only thing you have to take care of is the length order of the strings. In the tokenization function:\n",
    "\n",
    "1. Write a statement to order the strings in your vocabulary list,\n",
    "  * first by decreasing length, and then\n",
    "  * by alphabetic order.\n",
    "  \n",
    "  You will call this list `vocabulary_srt`; Knowing that, in the ASCII order, the upper case letters are placed before lower case ones, the list: ['D', 'e', 'sen', 'a', 's', 't']\n",
    "\n",
    "will be sorted as: ['sen', 'D', 'a', 'e', 's', 't']\n",
    "\n",
    "2. Escape the regular expression with `re.escape()` as some strings may include metacharacters, for instance 'a.', where the dot matches all the characters.\n",
    "3. Convert this list into a regular expression that results in a disjunction of subword tokens. Remember that the disjunction operator (or) for regular expressions is the vertical bar (`|`), as in `'a'|'b'`, meaning match `'a'` or `'b'`;\n",
    "3. Apply a regular expression function to tokenize your text: the corpus string. You will use `findall()`for this. You will return this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.escape('a.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "from functools import reduce \n",
    "def tokenize_bpe(corpus, vocabulary):\n",
    "    # sort tokens\n",
    "    v = list(vocabulary).copy()\n",
    "    v.sort(key=lambda x: (-len(x), x))\n",
    "    \n",
    "    pattern = reduce(lambda x, y: f'{x}|{re.escape(y)}', v)\n",
    "    return list(re.findall(pattern, corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize_bpe(corpus, vocabulary)[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now done with BPE and you can now consider the unigram language model.\n",
    "\n",
    "Read these two sections:\n",
    "\n",
    "1. Section 3.2 of _Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates_ (https://arxiv.org/pdf/1804.10959.pdf) by Kudo (2018).\n",
    "2. Section 2, algorithm 2 and the related text of _Byte Pair Encoding is Suboptimal for Language Model Pretraining_ (https://aclanthology.org/2020.findings-emnlp.414.pdf) by Bostrom and Durrett (2020).\n",
    "\n",
    "In your report, **summarize** (10 to 15 lines or so) with your own words the tokenization with a unigram language model as described by by Kudo (2018) and Bostrom and Durrett (2020). You will notably consider two aspects:\n",
    "1. How to obtain the subword vocabulary;\n",
    "2. How to tokenize a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your report, given what you have done on the byte-pair encoding, how would you build the “reasonably big seed vocabulary” needed for the unigram language model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the “reasonably big seed vocabulary”, you will now fit a unigram language model. You will start with a vocabulary of 50 subwords in addition to the character set and reduce it to 49, i.e. you will find one subword to discard.\n",
    "\n",
    "Kudo (2018) proposes the expectation-maximization algorithm that we have not seen in the course on natural language processing. Instead, in this lab, you will approximate the language model with the BPE algorithm.\n",
    "\n",
    "Write a `unigram_lm()` function that takes a corpus string and a vocabulary of subword tokens as input and returns a dictionary, where the keys are the subwords and each key value, the key relative frequency:\n",
    "\n",
    "    def unigram_lm(corpus, vocabulary):\n",
    "\n",
    "       ...\n",
    "\n",
    "      return unigram_probs\n",
    "Your function will:\n",
    "\n",
    "1. Tokenize your corpus with BPE (you can reuse the `tokenize_bpe()` function);\n",
    "2. Estimate the probability of each word (simply count the occurrences of the subwords and divide them by the length of the tokenized corpus);\n",
    "3. Return this model as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def unigram_lm(corpus, vocabulary):\n",
    "    tokenized_corpus = tokenize_bpe(corpus, vocabulary)\n",
    "\n",
    "    counter = Counter(tokenized_corpus)\n",
    "    \n",
    "    n = len(tokenized_corpus)\n",
    "    unigram_probs = {token: c / n for token, c in counter.items()}\n",
    "    return unigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_probs = unigram_lm(corpus, vocabulary)\n",
    "unigram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unigram_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now apply your unigram language model to tokenize a character sequence that does not include spaces, typically a single word in the Latin or Greek scripts or a sequence of words in Asian scripts, like Chinese or Korean.\n",
    "\n",
    "Write a `tokenize_lm()` function that takes a character sequence, `char_seq`, and a dictionary of unigram probabilities, `unigram_probs`,  as input and returns the subword tokens and the segmentation probability, (prob,tokens). You will only return the token list with the highest probability.\n",
    "\n",
    "    def tokenize_lm(char_seq, unigram_probs):\n",
    "\n",
    "      ...\n",
    "\n",
    "      return max(candidates)\n",
    "\n",
    "As an example, applying \n",
    "\n",
    "tokenize_lm('senare', unigram_probs)\n",
    "results in\n",
    "\n",
    "`(2.0899522820189735e-07, ['s', 'en', ar', 'e'])`\n",
    "\n",
    "Your function will cache (memoize) the results to speed up the computation. It will be similar to that of Norvig's in the notebook: How to Do Things with Words.ipynb. You can reuse it.\n",
    "Python has a built-in memoization function that you can use: @functools.lru_cache(maxsize=2**10). You can also use the newer @functools.cache() function if you have Python 3.9 or higher. See here: https://docs.python.org/3/library/functools.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from typing import List, Tuple\n",
    "\n",
    "def tokenize_lm(char_seq, unigram_probs):\n",
    "    \n",
    "    @functools.cache # Available from Python 3.9\n",
    "    def dp(i: int, progress: tuple) -> Tuple[Tuple[str], float]:\n",
    "        \n",
    "        if i >= n  : return 1, tuple()\n",
    "        if i == n-1: \n",
    "            progress = (char_seq[i], )\n",
    "            print(\"base case\", i, char_seq[i], progress)\n",
    "            return unigram_probs[char_seq[i]], progress\n",
    "        \n",
    "        dp1, best_seq1 = dp(-1, progress)\n",
    "        dp2, best_seq2 = dp(-1, progress[1:]) \n",
    "        \n",
    "        print(\"at\", char_seq[i])\n",
    "        print(f\"{best_seq1=}\")\n",
    "        print(f\"{best_seq2=}\")\n",
    "\n",
    "        this_token = char_seq[i]\n",
    "        print(f\"{this_token=}\")\n",
    "        \n",
    "        next_token = progress[0]\n",
    "        print(f\"{next_token=}\")\n",
    "        alt1 = unigram_probs[this_token] * dp1 if this_token in unigram_probs else float('-inf')\n",
    "        alt2 = unigram_probs[merged_token] * dp2 if (merged_token := this_token + next_token) in unigram_probs else float('-inf')        \n",
    "        \n",
    "        maxprob, best_sentence = (\n",
    "            (alt1, (this_token, ) + best_seq1) \n",
    "            if alt1 >= alt2 \n",
    "            else (alt2, (this_token + next_token, ) + best_seq2)\n",
    "        )\n",
    "        print(\"alt1\", (this_token, ) + best_seq1)\n",
    "        print(\"alt2\", (this_token + next_token, ) + best_seq2) \n",
    "        print(\"best_sentence\", best_sentence)\n",
    "        \n",
    "        progress = best_sentence \n",
    "        print(\"progress\", progress)\n",
    "                      \n",
    "        return maxprob, best_sentence\n",
    "    \n",
    "    progress = tuple()   \n",
    "    n = len(char_seq)\n",
    "    return dp(0, progress)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "    dp(i):\n",
    "        maximal probability of subword token in char_seq[i:]\n",
    "\n",
    "    compute:\n",
    "        dp(0)\n",
    "\n",
    "    base cases:\n",
    "        one char left: return unigram_probs of this token\n",
    "        outside: return 1\n",
    "\n",
    "\n",
    "    recursive step:\n",
    "        max(\n",
    "            prob(leave this token alone) + prob(rest),\n",
    "            prob(merge it with the one to the right) + prob(rest + 1 skip merged token)\n",
    "        )\n",
    "\n",
    "    a tt\n",
    "    \n",
    "    prob(att) * 1\n",
    "    \n",
    "    senare\n",
    "    ... a, r, e, ...\n",
    "    OR\n",
    "    ... ar, e, ...\n",
    "\n",
    "    r, e\n",
    "    r + e\n",
    "    re + 0\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from typing import List, Tuple\n",
    "\n",
    "def tokenize_lm(char_seq, unigram_probs):\n",
    "    \n",
    "    @functools.cache # Available from Python 3.9\n",
    "    def dp(i: int, progress: tuple) -> Tuple[float, Tuple[str]]:\n",
    "        \n",
    "        if i >= n  : return 1, tuple()\n",
    "        if i == n-1: \n",
    "            progress = tuple(char_seq[i]) + progress\n",
    "            #print(\"base case\", i, char_seq[i], progress)\n",
    "            return unigram_probs[char_seq[i]], progress\n",
    "        \n",
    "        dp1, best_seq1 = dp(i+1, progress)\n",
    "        dp2, best_seq2 = dp(i+2, progress) \n",
    "        \n",
    "        #print(\"at\", char_seq[i])\n",
    "        #print(f\"{best_seq1=}\")\n",
    "        #print(f\"{best_seq2=}\")\n",
    "\n",
    "        this_token = char_seq[i]\n",
    "        #print(f\"{this_token=}\")\n",
    "        next_token = best_seq1[0]\n",
    "        \n",
    "        alt1 = unigram_probs[this_token] * dp1 if this_token in unigram_probs else float('-inf')\n",
    "        alt2 = unigram_probs[merged_token] * dp2 if (merged_token := this_token + next_token) in unigram_probs else float('-inf')        \n",
    "        \n",
    "        maxprob, best_sentence = (\n",
    "            (alt1, (this_token, ) + best_seq1) \n",
    "            if alt1 >= alt2 \n",
    "            else (alt2, (this_token + next_token, ) + best_seq2)\n",
    "        )\n",
    "        #print(\"alt1\", (this_token, ) + best_seq1)\n",
    "        #print(\"alt2\", (this_token + next_token, ) + best_seq2) \n",
    "        #print(\"best_sentence\", best_sentence)\n",
    "        \n",
    "        progress = best_sentence \n",
    "        #print(\"progress\", progress)\n",
    "                      \n",
    "        return maxprob, progress\n",
    "    \n",
    "    progress = tuple()   \n",
    "    n = len(char_seq)\n",
    "    return dp(0, progress)\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    dp(i):\n",
    "        maximal probability of subword token in char_seq[i:]\n",
    "\n",
    "    compute:\n",
    "        dp(0)\n",
    "\n",
    "    base cases:\n",
    "        one char left: return unigram_probs of this token\n",
    "        outside: return 1\n",
    "\n",
    "\n",
    "    recursive step:\n",
    "        max(\n",
    "            prob(leave this token alone) + prob(rest),\n",
    "            prob(merge it with the one to the right) + prob(rest + 1 skip merged token)\n",
    "        )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "def splits_(text, start=0, L=20):\n",
    "    \"Return a list of all (first, rest) pairs; start <= len(first) <= L.\"\n",
    "    return [(text[:i], text[i:]) \n",
    "            for i in range(start, min(len(text), L)+1)]\n",
    "\n",
    "\n",
    "def segment(text, probs):\n",
    "    @functools.lru_cache(maxsize=2**10)\n",
    "    def _segment(text):\n",
    "        if not text: \n",
    "            return []\n",
    "        else:\n",
    "            candidates = ([first] + _segment(rest)\n",
    "                        for (first, rest) in splits_(text, 1))\n",
    "            return max(candidates, key=lambda x: Pwords(x, probs))\n",
    "        return _segment(text)\n",
    "\n",
    "\n",
    "def Pword(w, probs):\n",
    "    return 0 if w not in probs else probs[w] \n",
    "\n",
    "def Pwords(words, probs):\n",
    "    \"Probability of words, assuming each word is independent of others.\"\n",
    "    return product(Pword(w, probs) for w in words)\n",
    "\n",
    "def product(nums):\n",
    "    \"Multiply the numbers together.  (Like `sum`, but with multiplication.)\"\n",
    "    result = 1\n",
    "    for x in nums:\n",
    "        result *= x\n",
    "    return result\n",
    "\n",
    "def tokenize_lm(char_seq, probs):\n",
    "    # Use one of the two cache functions below to have a faster answer:  \n",
    "    \n",
    "    #@functools.cache # Available from Python 3.9\n",
    "    # The arguments of the cached function must be hashable that's why we define an inner cacheable function\n",
    "    @functools.lru_cache(maxsize=2**10)\n",
    "    def __tokenize_lm(char_seq):\n",
    "    # Write your code here\n",
    "        sequence = segment(char_seq, probs)\n",
    "        p = Pwords(sequence, probs)\n",
    "        return (p,  sequence) \n",
    "    \n",
    "    return __tokenize_lm(char_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenize_lm('senare', unigram_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_lm('att', unigram_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_probs[\"att\"], unigram_probs[\"a\"], unigram_probs[\"tt\"], unigram_probs[\"a\"] * unigram_probs[\"tt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Tokenization with Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous function applies to a sequence without spaces. You will now apply it to your corpus. Write a `tokenize_text_lm()` function that takes the whole `corpus` string as input and the unigram probabilities `unigram_probs` and return the corpus probability and the tokenized subwords. \n",
    "\n",
    "This function is just an application of the functions you just wrote, where you will:\n",
    "1. `split()` the string by whitespaces\n",
    "2. Break the tokens into subtokens and compute the probabilities of the resulting sequences;\n",
    "3. Sum the logarithm of these probabilities. Use log10 to check your output with the numbers in the notebook. \n",
    "\n",
    "It is very significant that you use the logarithm of the probabilities and the sum. If you multiply the probabilities, you will get an underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "def tokenize_text_lm(corpus, unigram_probs):\n",
    "    words = corpus.split(' ')\n",
    "    \n",
    "    tokens = []\n",
    "    log_probs = 0\n",
    "    i = 0\n",
    "    for word in tqdm.tqdm(words):\n",
    "        prob, word_tokens = tokenize_lm(word, unigram_probs)\n",
    "        \n",
    "        log_probs += np.log10(prob)\n",
    "        tokens += word_tokens\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(f\"{i=}, {log_probs=:.3f}, {word_tokens=}, {np.log10(prob)=:.3f}\")\n",
    "        i += 1\n",
    "\n",
    "    \n",
    "    return log_probs, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_loglikelihood, tokens = tokenize_text_lm(corpus, unigram_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_loglikelihood, tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now implement the final loop, where you will, at each iteration:\n",
    "1. Select one subword from the vocabulary.\n",
    "2. Compute the resulting log-likelihood of the corpus without this word.\n",
    "3. Compute the loss, i.e. the log-likelihood reduction when the subword is removed from the current vocabulary\n",
    "\n",
    "You will always keep the single characters in your vocabulary to avoid unknown words.\n",
    "\n",
    "Store the pairs, (log-likelihood, removed_subword) in a list `logloss_word` and rank them by likelihood value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def compute_logloss(corpus, vocabulary, init_loglikelihood):\n",
    "    logloss_word = []\n",
    "    \n",
    "    _vocabulary = vocabulary.copy()\n",
    "\n",
    "    for removed_token in _vocabulary:\n",
    "        if len(removed_token) == 1: continue\n",
    "            \n",
    "        new_vocabulary = vocabulary.copy()\n",
    "        new_vocabulary.remove(removed_token)\n",
    "        \n",
    "        #print(f\"{removed_token=}\")\n",
    "        #print(f\"{(removed_token in new_vocabulary)=}\")\n",
    "\n",
    "        _unigram_probs = unigram_lm(corpus, new_vocabulary)\n",
    "        #print(f\"{removed_token=}\")\n",
    "        logloss, _ = tokenize_text_lm(corpus, _unigram_probs)\n",
    "        logloss_word.append((logloss, removed_token))\n",
    "        #print(f\"{(logloss, removed_token)=}\")\n",
    "        \n",
    "    _ = [(logloss - init_loglikelihood, word) for logloss, word in logloss_word]\n",
    "    return sorted(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss_word = compute_logloss(corpus, vocabulary, init_loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will reduce now your vocabulary by one token: `out_candidate`. Write the piece of code to determine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "out_candidate = min(logloss_word, key=lambda x: logloss_word[0])\n",
    "print(f\"{logloss_word=}, {out_candidate=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_candidate = (-92.75720750979963, 'tt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested, you can improve this program and test it on larger corpora. You can also read a fine implementation of BPE by Andrej Karpathy: https://github.com/karpathy/minGPT/blob/master/mingpt/bpe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have written all the code and run all the cells, fill in your ID and as well as the name of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STIL_ID = [\"si7660da-s\", \"ni5324ro-s\"] # Write your stil ids as a list\n",
    "CURRENT_NOTEBOOK_PATH = os.path.join(os.getcwd(), \n",
    "                                     \"5-BPE_solution.ipynb\") # Write the name of your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission code will send your answer. It consists of the subword to discard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "ANSWER = json.dumps({'out_candidate': out_candidate})\n",
    "ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the moment of truth:\n",
    "1. Save your notebook and\n",
    "2. Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_NOTEBOOK_PATH = CURRENT_NOTEBOOK_PATH + \".submission.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "ASSIGNMENT = 5\n",
    "API_KEY = \"f581ba347babfea0b8f2c74a3a6776a7\"\n",
    "\n",
    "# Copy and compress current notebook\n",
    "with bz2.open(SUBMISSION_NOTEBOOK_PATH, mode=\"wb\") as fout:\n",
    "    with open(CURRENT_NOTEBOOK_PATH, \"rb\") as fin:\n",
    "        fout.write(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.post(\"https://vilde.cs.lth.se/edan20checker/submit\", \n",
    "                    files={\"notebook_file\": open(SUBMISSION_NOTEBOOK_PATH, \"rb\")}, \n",
    "                    data={\n",
    "                        \"stil_id\": STIL_ID,\n",
    "                        \"assignment\": ASSIGNMENT,\n",
    "                        \"answer\": ANSWER,\n",
    "                        \"api_key\": API_KEY,\n",
    "                    },\n",
    "               verify=True)\n",
    "\n",
    "# from IPython.display import display, JSON\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Turning in your assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your are done with the program. To complete this assignment, you will write a report where you will:\n",
    "1. Describe the background as well as the algorithms you used. For this, summarize the articles as described in the notebook:\n",
    "   * Preliminaries: subword tokenizers\n",
    "   * Design of the BPE Algorithm\n",
    "   * Unigram Language Model\n",
    "2. Describe your program as well as your results\n",
    "\n",
    "The whole report should be of 2 to 3 pages.\n",
    "\n",
    "Submit your report as well as your **notebook** (for archiving purposes) to Canvas: https://canvas.education.lu.se/. To write your report, you can either\n",
    "1. Write directly your text in Canvas, or\n",
    "2. Use Latex and Overleaf (www.overleaf.com). This will probably help you structure your text. You will then upload a PDF file in Canvas.\n",
    "\n",
    "The submission deadline is October 14, 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curious?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested, you can improve this program and test it on larger corpora. You can also read a fine implementation of BPE by Andrej Karpathy: https://github.com/karpathy/minGPT/blob/master/mingpt/bpe.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b97b11a820675205aae8f1d7f2a3f22bbd3a2c30189f44042310baf5b4cd1987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
